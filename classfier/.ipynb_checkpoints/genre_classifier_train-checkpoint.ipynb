{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eae6633-97bf-4ff1-bac9-d1cb8af8c653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch 1: Loss=0.6570, Acc=0.6467\n",
      "[Val]   Epoch 1: Acc=0.6784\n",
      "[Train] Epoch 2: Loss=0.6235, Acc=0.7139\n",
      "[Val]   Epoch 2: Acc=0.6935\n",
      "[Train] Epoch 3: Loss=0.6080, Acc=0.7089\n",
      "[Val]   Epoch 3: Acc=0.6884\n",
      "[Train] Epoch 4: Loss=0.5963, Acc=0.7306\n",
      "[Val]   Epoch 4: Acc=0.6985\n",
      "[Train] Epoch 5: Loss=0.5866, Acc=0.7261\n",
      "[Val]   Epoch 5: Acc=0.7085\n",
      "[Train] Epoch 6: Loss=0.5770, Acc=0.7339\n",
      "[Val]   Epoch 6: Acc=0.7286\n",
      "[Train] Epoch 7: Loss=0.5678, Acc=0.7317\n",
      "[Val]   Epoch 7: Acc=0.6985\n",
      "[Train] Epoch 8: Loss=0.5619, Acc=0.7372\n",
      "[Val]   Epoch 8: Acc=0.7136\n",
      "[Train] Epoch 9: Loss=0.5540, Acc=0.7456\n",
      "[Val]   Epoch 9: Acc=0.7236\n",
      "[Train] Epoch 10: Loss=0.5473, Acc=0.7467\n",
      "[Val]   Epoch 10: Acc=0.7186\n",
      "[Train] Epoch 11: Loss=0.5387, Acc=0.7572\n",
      "[Val]   Epoch 11: Acc=0.7136\n",
      "[Train] Epoch 12: Loss=0.5318, Acc=0.7544\n",
      "[Val]   Epoch 12: Acc=0.7085\n",
      "[Train] Epoch 13: Loss=0.5257, Acc=0.7589\n",
      "[Val]   Epoch 13: Acc=0.7588\n",
      "[Train] Epoch 14: Loss=0.5229, Acc=0.7611\n",
      "[Val]   Epoch 14: Acc=0.7538\n",
      "[Train] Epoch 15: Loss=0.5143, Acc=0.7656\n",
      "[Val]   Epoch 15: Acc=0.7487\n",
      "[Train] Epoch 16: Loss=0.5093, Acc=0.7672\n",
      "[Val]   Epoch 16: Acc=0.7588\n",
      "[Train] Epoch 17: Loss=0.5033, Acc=0.7767\n",
      "[Val]   Epoch 17: Acc=0.7236\n",
      "[Train] Epoch 18: Loss=0.5010, Acc=0.7661\n",
      "[Val]   Epoch 18: Acc=0.7538\n",
      "[Train] Epoch 19: Loss=0.4922, Acc=0.7789\n",
      "[Val]   Epoch 19: Acc=0.7638\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio as T\n",
    "import torchaudio.transforms as TT\n",
    "\n",
    "# --------------------------\n",
    "# 分类器模型\n",
    "# --------------------------\n",
    "class GenreClassifier(nn.Module):\n",
    "    def __init__(self, input_channels=80, ndf=64, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv1d(input_channels, ndf, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv1d(ndf, ndf * 2, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm1d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv1d(ndf * 2, ndf * 4, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm1d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "        )\n",
    "        self.fc = nn.Linear(ndf * 4, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "# --------------------------\n",
    "# 数据集类（支持标签）\n",
    "# --------------------------\n",
    "class LabeledAudioDataset(Dataset):\n",
    "    def __init__(self, root_dir, label_dict, split=\"train\",\n",
    "                 sr=22050, hop_samples=256, n_fft=1024, n_mels=80, target_time_steps=2580):\n",
    "        self.label_dict = label_dict\n",
    "        self.all_files = []\n",
    "        self.labels = []\n",
    "\n",
    "        for genre_name, label in label_dict.items():\n",
    "            subfolder = f\"{genre_name.lower()}_{split}\"\n",
    "            genre_path = os.path.join(root_dir, genre_name, subfolder)\n",
    "            files = [os.path.join(genre_path, f) for f in os.listdir(genre_path) if f.endswith(\".wav\")]\n",
    "            self.all_files.extend(files)\n",
    "            self.labels.extend([label] * len(files))\n",
    "\n",
    "        self.sr = sr\n",
    "        self.hop_samples = hop_samples\n",
    "        self.n_fft = n_fft\n",
    "        self.n_mels = n_mels\n",
    "        self.target_time_steps = target_time_steps\n",
    "\n",
    "        self.mel_transform = TT.MelSpectrogram(\n",
    "            sample_rate=sr,\n",
    "            win_length=hop_samples * 4,\n",
    "            hop_length=hop_samples,\n",
    "            n_fft=n_fft,\n",
    "            f_min=20.0,\n",
    "            f_max=sr / 2.0,\n",
    "            n_mels=n_mels,\n",
    "            power=1.0,\n",
    "            normalized=True,\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.all_files[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        audio, sr = T.load(file_path)\n",
    "        audio = torch.clamp(audio[0], -1.0, 1.0)\n",
    "\n",
    "        if sr != self.sr:\n",
    "            raise ValueError(f\"Unexpected sampling rate: {sr}\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            mel_spec = self.mel_transform(audio)\n",
    "            mel_spec = 20 * torch.log10(torch.clamp(mel_spec, min=1e-5)) - 20\n",
    "            mel_spec = torch.clamp((mel_spec + 100) / 100, 0.0, 1.0)\n",
    "\n",
    "        # Pad/crop to target length\n",
    "        time_steps = mel_spec.shape[1]\n",
    "        if time_steps < self.target_time_steps:\n",
    "            mel_spec = F.pad(mel_spec, (0, self.target_time_steps - time_steps))\n",
    "        else:\n",
    "            mel_spec = mel_spec[:, :self.target_time_steps]\n",
    "\n",
    "        return mel_spec, label\n",
    "\n",
    "# --------------------------\n",
    "# 训练函数\n",
    "# --------------------------\n",
    "def train_classifier(model, train_loader, val_loader, device, epochs=20, lr=1e-4):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0.0, 0, 0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(x)\n",
    "            loss = criterion(preds, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "            correct += (preds.argmax(dim=1) == y).sum().item()\n",
    "            total += x.size(0)\n",
    "\n",
    "        train_acc = correct / total\n",
    "        print(f\"[Train] Epoch {epoch+1}: Loss={total_loss/total:.4f}, Acc={train_acc:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                preds = model(x)\n",
    "                correct += (preds.argmax(dim=1) == y).sum().item()\n",
    "                total += x.size(0)\n",
    "\n",
    "        val_acc = correct / total\n",
    "        print(f\"[Val]   Epoch {epoch+1}: Acc={val_acc:.4f}\")\n",
    "\n",
    "# --------------------------\n",
    "# 主程序\n",
    "# --------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    root_dir = \"/home/quincy/DATA/music/dataset/wav_fma_split\"\n",
    "    label_dict = {\"Pop\": 0, \"Rock\": 1}  # 根据你的类别扩展即可\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    train_dataset = LabeledAudioDataset(root_dir=root_dir, label_dict=label_dict, split=\"train\")\n",
    "    val_dataset = LabeledAudioDataset(root_dir=root_dir, label_dict=label_dict, split=\"val\")\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=4)\n",
    "\n",
    "    model = GenreClassifier(input_channels=80, num_classes=len(label_dict))\n",
    "    train_classifier(model, train_loader, val_loader, device, epochs=100, lr=1e-4)\n",
    "\n",
    "    # 可选：保存模型\n",
    "    torch.save(model.state_dict(), \"genre_classifier1.pth\")\n",
    "    print(\"模型已保存为 genre_classifier.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466ddee5-2a87-4bac-baf2-f686198c3aa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
