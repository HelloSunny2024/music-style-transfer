{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eae6633-97bf-4ff1-bac9-d1cb8af8c653",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio as T\n",
    "import torchaudio.transforms as TT\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Genre Classifier Model\n",
    "class GenreClassifier(nn.Module):\n",
    "    def __init__(self, input_channels=80, ndf=64, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv1d(input_channels, ndf, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv1d(ndf, ndf * 2, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm1d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv1d(ndf * 2, ndf * 4, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm1d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "        )\n",
    "        self.fc = nn.Linear(ndf * 4, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "\n",
    "# Dataset Class (with labels)\n",
    "class LabeledAudioDataset(Dataset):\n",
    "    def __init__(self, root_dir, label_dict, split=\"train\",\n",
    "                 sr=22050, hop_samples=256, n_fft=1024, n_mels=80, target_time_steps=2580):\n",
    "        self.label_dict = label_dict\n",
    "        self.all_files = []\n",
    "        self.labels = []\n",
    "\n",
    "        for genre_name, label in label_dict.items():\n",
    "            subfolder = f\"{genre_name.lower()}_{split}\"\n",
    "            genre_path = os.path.join(root_dir, genre_name, subfolder)\n",
    "            files = [os.path.join(genre_path, f) for f in os.listdir(genre_path) if f.endswith(\".wav\")]\n",
    "            self.all_files.extend(files)\n",
    "            self.labels.extend([label] * len(files))\n",
    "\n",
    "        self.sr = sr\n",
    "        self.hop_samples = hop_samples\n",
    "        self.n_fft = n_fft\n",
    "        self.n_mels = n_mels\n",
    "        self.target_time_steps = target_time_steps\n",
    "\n",
    "        self.mel_transform = TT.MelSpectrogram(\n",
    "            sample_rate=sr,\n",
    "            win_length=hop_samples * 4,\n",
    "            hop_length=hop_samples,\n",
    "            n_fft=n_fft,\n",
    "            f_min=20.0,\n",
    "            f_max=sr / 2.0,\n",
    "            n_mels=n_mels,\n",
    "            power=1.0,\n",
    "            normalized=True,\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.all_files[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        audio, sr = T.load(file_path)\n",
    "        audio = torch.clamp(audio[0], -1.0, 1.0)\n",
    "\n",
    "        if sr != self.sr:\n",
    "            raise ValueError(f\"Unexpected sampling rate: {sr}\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            mel_spec = self.mel_transform(audio)\n",
    "            mel_spec = 20 * torch.log10(torch.clamp(mel_spec, min=1e-5)) - 20\n",
    "            mel_spec = torch.clamp((mel_spec + 100) / 100, 0.0, 1.0)\n",
    "\n",
    "        # Pad or crop to target length\n",
    "        time_steps = mel_spec.shape[1]\n",
    "        if time_steps < self.target_time_steps:\n",
    "            mel_spec = F.pad(mel_spec, (0, self.target_time_steps - time_steps))\n",
    "        else:\n",
    "            mel_spec = mel_spec[:, :self.target_time_steps]\n",
    "\n",
    "        return mel_spec, label\n",
    "\n",
    "# Training Function\n",
    "def train_classifier(model, train_loader, val_loader, device, epochs=20, lr=1e-4):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    best_val_acc = 0.0  \n",
    "    best_model_wts = None  \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0.0, 0, 0\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False)\n",
    "\n",
    "        for x, y in pbar:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(x)\n",
    "            loss = criterion(preds, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "            correct += (preds.argmax(dim=1) == y).sum().item()\n",
    "            total += x.size(0)\n",
    "\n",
    "            pbar.set_postfix(loss=total_loss/total, acc=correct/total)\n",
    "\n",
    "        train_acc = correct / total\n",
    "        train_loss = total_loss / total\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                preds = model(x)\n",
    "                correct += (preds.argmax(dim=1) == y).sum().item()\n",
    "                total += x.size(0)\n",
    "\n",
    "        val_acc = correct / total\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        print(f\"[Train] Epoch {epoch+1}: Loss={train_loss:.4f}, Acc={train_acc:.4f}\")\n",
    "        print(f\"[Val]   Epoch {epoch+1}: Acc={val_acc:.4f}\")\n",
    "\n",
    "        # Save best model based on validation accuracy\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_wts = model.state_dict()\n",
    "            torch.save(best_model_wts, \"best_genre_classifier.pth\")\n",
    "            print(f\"Best model saved with validation accuracy: {val_acc:.4f}\")\n",
    "\n",
    "    # Plot loss and validation accuracy curves\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(epochs), train_losses, label=\"Train Loss\")\n",
    "    plt.title(\"Train Loss Curve\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(epochs), val_accuracies, label=\"Validation Accuracy\")\n",
    "    plt.title(\"Validation Accuracy Curve\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root_dir = \"/DATA/music/dataset/wav_fma_split\"\n",
    "    label_dict = {\"Pop\": 0, \"Rock\": 1}  # Extend this dict according to your genre labels\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    train_dataset = LabeledAudioDataset(root_dir=root_dir, label_dict=label_dict, split=\"train\")\n",
    "    val_dataset = LabeledAudioDataset(root_dir=root_dir, label_dict=label_dict, split=\"val\")\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4)\n",
    "\n",
    "    model = GenreClassifier(input_channels=80, num_classes=len(label_dict))\n",
    "    train_classifier(model, train_loader, val_loader, device, epochs=100, lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466ddee5-2a87-4bac-baf2-f686198c3aa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
